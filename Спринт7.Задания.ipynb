{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0023f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Java runtime present, requesting install.\n",
      "/Users/user/opt/anaconda3/lib/python3.9/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLearning DataFrames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-01-04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3744\u001b[39m, \u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m322\u001b[39m),\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-01-04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2434\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m382\u001b[39m),\n\u001b[1;32m     11\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-01-04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2434\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m159\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-01-04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5677\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m499\u001b[39m)\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [('2021-01-04', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-04', 3744, 32, 159),\n",
    "        ('2021-01-04', 4342, 32, 159),\n",
    "        ('2021-01-04', 4342, 12, 259),\n",
    "        ('2021-01-04', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "]\n",
    "\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "df = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "+-------+---+\n",
    "|   Name|Age|\n",
    "+-------+---+\n",
    "|    Max| 55|\n",
    "|    Yan| 53|\n",
    "| Dmitry| 54|\n",
    "|    Ann| 25|\n",
    "+-------+---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405c92ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHADOOP_CONF_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/etc/hadoop/conf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYARN_CONF_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/etc/hadoop/conf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m      6\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      7\u001b[0m findspark\u001b[38;5;241m.\u001b[39mfind()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"delendikms_dataframe_task_1\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [('Max', 55),\n",
    "        ('Yan', 53),\n",
    "        ('Dmitry', 54),\n",
    "        ('Ann', 25)\n",
    "]\n",
    "\n",
    "columns = ['Name', 'Age']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"delendikms_dataframe_task_2\") \\\n",
    "                    .getOrCreate()\n",
    "usersDF = spark.read.load(path = \"/user/master/data/events/date=2022-05-25\", format = 'json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a791cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "        .config(\"spark.driver.cores\", \"2\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .appName(\"df_task2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787bd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.read.load(path = \"/user/master/data/events/date=2022-05-25\", format = 'json')\n",
    "usersDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b9c67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1633964557.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    hadoop fs -ls /user/master/data/snapshots\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/master/data/snapshots/channels/actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"/user/master/data/snapshots/channels/actual/part-00000-beae86b3-af54-4415-8f29-31dd79cfe178-c000.snappy.parquet\")\n",
    "#df.show()\n",
    "\n",
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"channel_type\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(\"/user/msdelendik/analytics/test\")\n",
    "\n",
    "df_new = spark.read.parquet(\"/user/msdelendik/analytics/test\")\n",
    "df_new.select(\"channel_type\").orderBy(\"channel_type\").distinct().show()\n",
    "\n",
    "#df.write.format('csv').mode('overwrite').save('dataseets/covid19_dataset/time_province_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма \n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "        ( 322, \"1\"),\n",
    "        ( 250, \"2\" ),\n",
    "        (400, \"2\"),\n",
    "        (159, \"1\"),\n",
    "        (382, \"2\"),\n",
    "        (322, \"1\")\n",
    "]\n",
    "# названия атрибутов\n",
    "columns = ['title', 'author', 'book_id']\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=columns)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n",
    "# напишите ваш код ниже\n",
    "df_join =df.join(df_library, ['book_id'], 'anti')\n",
    "df_join.select('title').distinct().show(10,False)\n",
    "df_join.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cba0fe",
   "metadata": {},
   "source": [
    "## book_1 = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "book_2 = [\n",
    "        ('Black Beauty',657 ,'Anna Sewell'),\n",
    "        ('Artemis Fowl',558,'Eoin Colfer'),\n",
    "        ('The Magic Faraway Tree', 567,'Enid Blyton'),\n",
    "        ('The Witches', 567,'Roald Dahl'),\n",
    "        ('Frankenstein',567 ,'Mary Shelley'),\n",
    "        ('The Little Prince',557 ,'Antoine de Saint-Exupéry'),\n",
    "        ('The Truth', 576 ,'Terry Pratchett')\n",
    "]\n",
    "# названия атрибутов\n",
    "columns_1= ['title', 'author', 'book_id']\n",
    "columns_2 = ['title', 'book_id', 'author']\n",
    "# создаём датафреймы\n",
    "df_1 = spark.createDataFrame(data=book_1 , schema=columns_1)\n",
    "df_2  = spark.createDataFrame(data=book_2 , schema=columns_2)\n",
    "# напишите ваш код ниже\n",
    "df_1.unionByName(df_2).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef24b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b967cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"hdfs://rc1a-dataproc-m-dg5lgqqm7jju58f9.mdb.yandexcloud.net//user/msdelendik/analytics/test_check\")\n",
    "df_join.checkpoint()\n",
    "df_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.read.load(path = \"hdfs://rc1a-dataproc-m-dg5lgqqm7jju58f9.mdb.yandexcloud.net//user/master/data/events/date=2022-05-31\", format = 'json')\n",
    "usersDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "usersDF = spark.read.load(path = \"hdfs://rc1a-dataproc-m-dg5lgqqm7jju58f9.mdb.yandexcloud.net//user/master/data/events/date=2022-05-31\", format = 'JSON')\n",
    "usersDF.withColumn('hour', F.hour(F.col(\"event.datetime\"))) \\\n",
    "        .withColumn('minute', F.minute(F.col(\"event.datetime\"))) \\\n",
    "        .withColumn('second', F.second(F.col(\"event.datetime\"))) \\\n",
    "        .orderBy(F.col(\"event.datetime\").desc()) \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF.filter(F.col(\"event.message_to\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_react_df = spark.read.load(path = \"hdfs://rc1a-dataproc-m-dg5lgqqm7jju58f9.mdb.yandexcloud.net//user/master/data/events/date=2022-05-25\", format = 'JSON')\n",
    "#max_react_df.select(F.col(\"event_type\")).distinct().show(10, False)\n",
    "#max_react_df.select(F.col(\"event.reaction_type\")).distinct().show(10, False)\n",
    "df_2 = max_react_df.filter(F.col(\"event_type\")==\"reaction\").groupBy(F.col(\"event.reaction_from\")).count()\n",
    "df_2.select(F.max(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "# данные  датафрейма \n",
    "data = [('2021-01-06', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-05', 3744, 32, 159),\n",
    "        ('2021-01-06', 4342, 32, 159),\n",
    "        ('2021-01-05', 4342, 12, 259),\n",
    "        ('2021-01-06', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "]\n",
    "# названия атрибутов\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "# создаём датафрейм\n",
    "df = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a831c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем оконную функцию и модуль Spark Functions\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# создаём объект оконной функции\n",
    "window = Window().partitionBy(['dt']).orderBy(F.asc('purchase_amount'))\n",
    "\n",
    "# создаём колонку с рассчитанной статистикой по оконной функции\n",
    "df_window = df.withColumn(\"rank\", F.rank().over(window))\n",
    "\n",
    "# выводим нужные колонки\n",
    "df_window.select('dt', 'user_id', 'rank', 'purchase_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb839c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём объект оконной функции\n",
    "window_2 = Window().orderBy(F.asc('purchase_amount'))\n",
    "\n",
    "# создаём колонку с рассчитанной статистикой по оконной функции\n",
    "df_window = df.withColumn(\"row_number\", F.row_number().over(window_2))\n",
    "\n",
    "# выводим нужные колонки\n",
    "df_window.select('dt', 'user_id', 'purchase_amount', 'row_number').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1aafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = spark.read.json(\"/user/master/data/events/date=2022-05-01\")\n",
    "# Выведите схему датафрейма ниже\n",
    "events.printSchema()\n",
    "#events.withColumn(\"data\", F.explode(\"event\")).show()\n",
    "#events.select(F.col(\"event\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window().partitionBy(\"event.message_from\").orderBy('event.user')\n",
    "\n",
    "dfWithLag = events.withColumn(\"lag_7\",F.lag(\"event.message_to\", 7).over(window))\n",
    "\n",
    "dfWithLag.select(\"event.message_from\", \"lag_7\") \\\n",
    ".filter(dfWithLag.lag_7.isNotNull()) \\\n",
    ".orderBy(F.desc(\"event.message_from\")) \\\n",
    ".show(10, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62526162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('2021-01-06', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-05', 3744, 32, 159),\n",
    "        ('2021-01-06', 4342, 32, 159),\n",
    "        ('2021-01-05', 4342, 12, 259),\n",
    "        ('2021-01-06', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "]\n",
    "\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "window = Window().partitionBy(\"user_id\")\n",
    "\n",
    "df_min_max = df.withColumn(\"min\", F.min(\"purchase_amount\").over(window)) \\\n",
    "            .withColumn(\"max\", F.max(\"purchase_amount\").over(window))\n",
    "\n",
    "df_min_max.select(\"user_id\", \"max\", \"min\") \\\n",
    "            .distinct() \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = spark.read.json(\"/user/master/data/events\")\n",
    "events.write \\\n",
    "        .partitionBy(\"date\", \"event_type\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"/user/msdelendik/data/events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10491cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/user/msdelendik/data/events\").orderBy(F.col('event.datetime').desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb77fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем системную библиотеку\n",
    "import sys\n",
    "\n",
    "# Объявляем функцию main, которая будет выполнять джобу. \n",
    "# Так код будет удобно и правильно оформлен.\n",
    "def main():\n",
    "                client_name = sys.argv[1] # параметризуем имя клиента\n",
    "        date = sys.argv[2] # параметризуем дату\n",
    "        dir_name = sys.argv[3] # параметризуем директорию\n",
    "\n",
    "                # Здесь будет основная часть кода\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    " \n",
    "def main():\n",
    "        date = sys.argv[1]\n",
    "        base_input_path = sys.argv[2]\n",
    "        base_output_path = sys.argv[3]\n",
    "\n",
    "        conf = SparkConf().setAppName(f\"EventsPartitioningJob-{date}\")\n",
    "        sc = SparkContext(conf=conf)\n",
    "        sql = SQLContext(sc)\n",
    "\n",
    " # Напишите директорию чтения в общем виде\n",
    "        events = sql.read.json(f\"{base_input_path}/date={date}\")\n",
    "\n",
    "# Напишите директорию записи\n",
    "        events\\\n",
    "        .write\\\n",
    "        .partitionBy(\"event_type\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(f\"{base_output_path}/date={date}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ebafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "dt = '2023-05-06' \n",
    "#dt_1 = datetime.fromisoformat(dt).date() - 8\n",
    "dt_1 = datetime.strptime(dt, '%Y-%m-%d') - timedelta(days=1)\n",
    "#dt_1 = datetime.date(dt, '%y/%m/%d') - 8 \n",
    "print(dt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98154c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/user/msdelendik/data/events/date=2023-05-07/event_type=message', '/user/msdelendik/data/events/date=2023-05-06/event_type=message', '/user/msdelendik/data/events/date=2023-05-05/event_type=message', '/user/msdelendik/data/events/date=2023-05-04/event_type=message', '/user/msdelendik/data/events/date=2023-05-03/event_type=message', '/user/msdelendik/data/events/date=2023-05-02/event_type=message', '/user/msdelendik/data/events/date=2023-05-01/event_type=message', '/user/msdelendik/data/events/date=2023-04-30/event_type=message']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def input_paths (date: str, depth: int):\n",
    "    path_list = []\n",
    "    for i in range (depth):\n",
    "        dt = datetime.strptime(date, '%Y-%m-%d') - timedelta(days=i)\n",
    "        dt = dt.date()\n",
    "        path = f\"/user/msdelendik/data/events/date={dt}/event_type=message\"\n",
    "        path_list.append(path)\n",
    "    print(path_list)\n",
    "#     if depth is not None:\n",
    "#         path_list.append(path)\n",
    "#     if depth is None: \n",
    "#             raise\n",
    "\n",
    "input_paths ('2023-05-07', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e2bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: hdfs: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls \"/user/msdelendik/data/events/date=yyyy-MM-dd/event_type=message\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
